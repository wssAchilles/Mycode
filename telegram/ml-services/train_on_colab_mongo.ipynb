{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Recsys Artifacts on MongoDB (Two-Tower + Phoenix + FAISS)\n",
        "\n",
        "这个 Notebook 用于 **训练你自己的语料（MongoDB posts/user_actions）**，并生成可直接发布到 GCS 的 artifacts，使线上 `ml-services` 的 `ANN` 返回 **Mongo `posts._id` (ObjectId string)**，从而真正打通 OON（Out-of-Network）召回。\n",
        "\n",
        "## 你是否需要 GPU？\n",
        "\n",
        "- **必须做**：Two-Tower + FAISS（否则 ANN 仍然会返回 `Nxxxxx` 这类外部语料 ID，无法 hydrate）。\n",
        "- Two-Tower 训练：小规模数据 **CPU 可跑**，但会慢；建议用 GPU（A100/T4 都行）。\n",
        "- Phoenix 训练：Transformer 结构，**强烈建议 GPU**；同时建议用更小的模型配置以满足 Cloud Run CPU 推理的 p95 目标。\n",
        "\n",
        "## 输出文件（与服务端加载路径一致）\n",
        "\n",
        "- `data/news_vocab.pkl`：key 必须是 **Mongo ObjectId string**\n",
        "- `data/user_vocab.pkl`\n",
        "- `data/item_embeddings.npy`\n",
        "- `models/two_tower_epoch_latest.pt`\n",
        "- `models/phoenix_epoch_latest.pt`（可选）\n",
        "- `models/faiss_ivf_pq.index`\n",
        "- `models/faiss_id_mapping.pkl`\n",
        "\n",
        "> 最后会生成一个 `stage/` 目录，结构为：`two_tower/model.pt`、`phoenix/model.pt`、`faiss/*`、`data/*`，用于你本地 `gcloud storage cp -r` 上传到 `gs://telegram-467705-recsys/artifacts/$ARTIFACT_VERSION/`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 0) 环境检查 / GPU 检测 ---\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "print('python:', sys.version)\n",
        "print('platform:', platform.platform())\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print('torch:', torch.__version__)\n",
        "    print('cuda available:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('cuda:', torch.version.cuda)\n",
        "        print('gpu:', torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print('torch import failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1) 安装依赖（Colab 一般已经有 torch/numpy；这里只补齐缺的） ---\n",
        "# 说明：faiss-cpu 用于构建索引；训练本身不依赖 faiss。\n",
        "# 如遇到安装冲突，可先重启运行时再执行。\n",
        "\n",
        "!pip -q install \"pymongo>=4.6\" \"tqdm>=4.65\" \"faiss-cpu>=1.7.4\" \"google-cloud-storage>=2.16\" \"python-dotenv>=1.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2) 配置参数（按你的小规模目标做默认值；可自行调整） ---\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "@dataclass\n",
        "class Cfg:\n",
        "    # 数据时间窗：用于导出训练数据\n",
        "    days_posts: int = 30\n",
        "    days_actions: int = 30\n",
        "\n",
        "    # Two-Tower\n",
        "    two_tower_embedding_dim: int = 256\n",
        "    max_history: int = 100\n",
        "    neg_per_pos: int = 4\n",
        "    batch_size: int = 2048\n",
        "    epochs: int = 3\n",
        "    lr: float = 1e-3\n",
        "\n",
        "    # Phoenix（可选，默认训练一个小模型，适合 Cloud Run CPU 推理）\n",
        "    train_phoenix: bool = True\n",
        "    phoenix_embedding_dim: int = 256\n",
        "    phoenix_num_heads: int = 8\n",
        "    phoenix_num_layers: int = 4\n",
        "    phoenix_batch_size: int = 512\n",
        "    phoenix_epochs: int = 2\n",
        "    phoenix_lr: float = 5e-4\n",
        "\n",
        "    # 输出目录（与 ml-services 目录结构一致）\n",
        "    out_root: Path = Path('.')\n",
        "\n",
        "cfg = Cfg()\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3) 读取 MongoDB（用 getpass 输入，避免把密码写进 Notebook） ---\n",
        "import getpass\n",
        "from pymongo import MongoClient\n",
        "\n",
        "if not os.environ.get('MONGODB_URI'):\n",
        "    os.environ['MONGODB_URI'] = getpass.getpass('MONGODB_URI (Mongo Atlas URI): ')\n",
        "\n",
        "mongo_uri = os.environ['MONGODB_URI']\n",
        "client = MongoClient(mongo_uri)\n",
        "db = client.get_default_database()\n",
        "if db is None:\n",
        "    # 如果 URI 没写默认库名，就手动指定\n",
        "    db_name = os.environ.get('MONGODB_DB') or os.environ.get('MONGODB_DATABASE') or getpass.getpass('Mongo DB name: ')\n",
        "    db = client[db_name]\n",
        "\n",
        "print('db:', db.name)\n",
        "posts_col = db['posts']\n",
        "actions_col = db['user_actions']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4) 拉取 posts / user_actions（小规模可直接拉入内存；大了就要分批/落盘） ---\n",
        "from tqdm import tqdm\n",
        "\n",
        "now = datetime.now(timezone.utc)\n",
        "cutoff_posts = (now - timedelta(days=cfg.days_posts)).replace(tzinfo=None)\n",
        "cutoff_actions = (now - timedelta(days=cfg.days_actions)).replace(tzinfo=None)\n",
        "\n",
        "# 只训练非新闻、未删除的帖子\n",
        "posts_cursor = posts_col.find(\n",
        "    {\n",
        "        'deletedAt': None,\n",
        "        'isNews': {'$ne': True},\n",
        "        'createdAt': {'$gte': cutoff_posts},\n",
        "    },\n",
        "    {'_id': 1, 'authorId': 1, 'createdAt': 1, 'engagementScore': 1}\n",
        ")\n",
        "posts = list(posts_cursor)\n",
        "post_ids = [str(p['_id']) for p in posts]\n",
        "post_id_set = set(post_ids)\n",
        "print('posts:', len(post_ids))\n",
        "\n",
        "# 行为：用于训练（注意：你现在的前端已做“进入视口”曝光；数据质量较好）\n",
        "actions_cursor = actions_col.find(\n",
        "    {\n",
        "        'timestamp': {'$gte': cutoff_actions},\n",
        "        'targetPostId': {'$exists': True, '$ne': None},\n",
        "        'action': {'$in': ['impression', 'click', 'like', 'reply', 'repost', 'quote', 'delivery']},\n",
        "    },\n",
        "    {'userId': 1, 'action': 1, 'targetPostId': 1, 'timestamp': 1, 'requestId': 1}\n",
        ")\n",
        "\n",
        "actions = []\n",
        "for a in actions_cursor:\n",
        "    pid = str(a.get('targetPostId')) if a.get('targetPostId') is not None else None\n",
        "    if not pid or pid not in post_id_set:\n",
        "        continue\n",
        "    uid = str(a.get('userId') or '')\n",
        "    if not uid:\n",
        "        continue\n",
        "    actions.append({\n",
        "        'userId': uid,\n",
        "        'action': str(a.get('action')),\n",
        "        'postId': pid,\n",
        "        'ts': a.get('timestamp'),\n",
        "        'requestId': a.get('requestId'),\n",
        "    })\n",
        "\n",
        "print('actions:', len(actions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5) 构建 vocab（关键：news_vocab 的 key 必须是 Mongo ObjectId string） ---\n",
        "import pickle\n",
        "\n",
        "DATA_DIR = cfg.out_root / 'data'\n",
        "MODELS_DIR = cfg.out_root / 'models'\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# news_vocab: postId -> index\n",
        "news_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "for pid in sorted(post_id_set):\n",
        "    news_vocab[pid] = len(news_vocab)\n",
        "\n",
        "# user_vocab: userId -> index\n",
        "user_ids = sorted({a['userId'] for a in actions})\n",
        "user_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "for uid in user_ids:\n",
        "    user_vocab[uid] = len(user_vocab)\n",
        "\n",
        "with open(DATA_DIR / 'news_vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(news_vocab, f)\n",
        "with open(DATA_DIR / 'user_vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(user_vocab, f)\n",
        "\n",
        "print('news_vocab size:', len(news_vocab))\n",
        "print('user_vocab size:', len(user_vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6) 构建训练样本（Two-Tower pointwise + 负采样） ---\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "POS_ACTIONS = {'click', 'like', 'reply', 'repost', 'quote'}\n",
        "\n",
        "# 按 user 分组并按时间排序\n",
        "by_user = defaultdict(list)\n",
        "for a in actions:\n",
        "    by_user[a['userId']].append((a['ts'], a['action'], a['postId']))\n",
        "for uid in list(by_user.keys()):\n",
        "    by_user[uid].sort(key=lambda x: x[0] or datetime.min)\n",
        "\n",
        "all_posts_list = list(post_id_set)\n",
        "\n",
        "samples = []\n",
        "\n",
        "for uid, events in tqdm(by_user.items(), desc='build samples'):\n",
        "    history = []\n",
        "    history_set = set()\n",
        "\n",
        "    for ts, act, pid in events:\n",
        "        # 正样本\n",
        "        if act in POS_ACTIONS:\n",
        "            samples.append({\n",
        "                'user_id': uid,\n",
        "                'history': history[-cfg.max_history:],\n",
        "                'candidate_id': pid,\n",
        "                'label': 1.0,\n",
        "            })\n",
        "\n",
        "            # 负采样\n",
        "            for _ in range(cfg.neg_per_pos):\n",
        "                for _try in range(20):\n",
        "                    neg = random.choice(all_posts_list)\n",
        "                    if neg == pid:\n",
        "                        continue\n",
        "                    if neg in history_set:\n",
        "                        continue\n",
        "                    samples.append({\n",
        "                        'user_id': uid,\n",
        "                        'history': history[-cfg.max_history:],\n",
        "                        'candidate_id': neg,\n",
        "                        'label': 0.0,\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "        # 更新历史（把所有曝光/点击等都加入历史，避免只看强正反馈导致稀疏）\n",
        "        if pid not in history_set:\n",
        "            history.append(pid)\n",
        "            history_set.add(pid)\n",
        "\n",
        "print('total samples:', len(samples))\n",
        "random.shuffle(samples)\n",
        "\n",
        "split = int(len(samples) * 0.95)\n",
        "train_samples = samples[:split]\n",
        "val_samples = samples[split:]\n",
        "\n",
        "print('train:', len(train_samples), 'val:', len(val_samples))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7) Two-Tower 训练（GPU 推荐；CPU 也能跑但更慢） ---\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 优先使用仓库内实现；如果你没有 clone 仓库，则下一格会提供 fallback 实现\n",
        "using_repo_model = False\n",
        "try:\n",
        "    from scripts.model_arch import TwoTowerModel  # type: ignore\n",
        "    using_repo_model = True\n",
        "    print('✅ Using TwoTowerModel from scripts.model_arch')\n",
        "except Exception as e:\n",
        "    print('⚠️ Cannot import scripts.model_arch.TwoTowerModel, will use fallback. err=', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果你在 Colab 里 `git clone` 了仓库并把工作目录切到 `telegram/ml-services/`，上一格会自动 import `scripts.model_arch.TwoTowerModel`。\n",
        "\n",
        "如果 import 失败（比如你没 clone 仓库），下一格会定义一个 **与服务端一致** 的 fallback 版本 TwoTower 模型结构。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7.1) TwoTowerModel fallback（当无法 import scripts.model_arch 时） ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "if using_repo_model:\n",
        "    print('✅ Repo TwoTowerModel already loaded, skip fallback')\n",
        "else:\n",
        "    class _NewsEncoder(nn.Module):\n",
        "        def __init__(self, num_news, embedding_dim):\n",
        "            super().__init__()\n",
        "            self.news_embedding = nn.Embedding(num_news, embedding_dim)\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(embedding_dim, embedding_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(embedding_dim, embedding_dim),\n",
        "            )\n",
        "\n",
        "        def forward(self, news_ids):\n",
        "            emb = self.news_embedding(news_ids)\n",
        "            out = self.fc(emb)\n",
        "            return F.normalize(out, p=2, dim=1)\n",
        "\n",
        "    class _UserEncoder(nn.Module):\n",
        "        def __init__(self, num_users, news_encoder, embedding_dim):\n",
        "            super().__init__()\n",
        "            self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "            self.news_encoder = news_encoder\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(embedding_dim, embedding_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(embedding_dim, embedding_dim),\n",
        "            )\n",
        "\n",
        "        def forward(self, user_ids, history_news_ids, history_mask):\n",
        "            user_emb = self.user_embedding(user_ids)\n",
        "            hist_emb = self.news_encoder.news_embedding(history_news_ids)\n",
        "            mask = history_mask.unsqueeze(-1)\n",
        "            hist_emb = hist_emb * mask\n",
        "            sum_emb = hist_emb.sum(dim=1)\n",
        "            count = mask.sum(dim=1).clamp(min=1)\n",
        "            avg_hist = sum_emb / count\n",
        "            combined = user_emb + avg_hist\n",
        "            out = self.fc(combined)\n",
        "            return F.normalize(out, p=2, dim=1)\n",
        "\n",
        "    class TwoTowerModel(nn.Module):\n",
        "        def __init__(self, num_users, num_news, embedding_dim):\n",
        "            super().__init__()\n",
        "            self.news_encoder = _NewsEncoder(num_news, embedding_dim)\n",
        "            self.user_encoder = _UserEncoder(num_users, self.news_encoder, embedding_dim)\n",
        "\n",
        "        def forward(self, user_ids, history_news_ids, history_mask, target_news_ids):\n",
        "            user_vec = self.user_encoder(user_ids, history_news_ids, history_mask)\n",
        "            item_vec = self.news_encoder(target_news_ids)\n",
        "            logits = (user_vec * item_vec).sum(dim=1)\n",
        "            return logits, user_vec, item_vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7.2) Dataset / DataLoader ---\n",
        "class TwoTowerDataset(Dataset):\n",
        "    def __init__(self, samples, news_vocab, user_vocab, max_history_len):\n",
        "        self.samples = samples\n",
        "        self.news_vocab = news_vocab\n",
        "        self.user_vocab = user_vocab\n",
        "        self.max_history_len = max_history_len\n",
        "        self.unk_news = news_vocab.get('<UNK>', 1)\n",
        "        self.unk_user = user_vocab.get('<UNK>', 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        uid = self.user_vocab.get(s['user_id'], self.unk_user)\n",
        "        hist = [self.news_vocab.get(pid, self.unk_news) for pid in (s.get('history') or [])]\n",
        "        if len(hist) > self.max_history_len:\n",
        "            hist = hist[-self.max_history_len:]\n",
        "            mask = [1.0] * self.max_history_len\n",
        "        else:\n",
        "            pad = self.max_history_len - len(hist)\n",
        "            mask = [1.0] * len(hist) + [0.0] * pad\n",
        "            hist = hist + [0] * pad\n",
        "\n",
        "        target = self.news_vocab.get(s['candidate_id'], self.unk_news)\n",
        "        label = float(s['label'])\n",
        "\n",
        "        return (\n",
        "            torch.tensor(uid, dtype=torch.long),\n",
        "            torch.tensor(hist, dtype=torch.long),\n",
        "            torch.tensor(mask, dtype=torch.float),\n",
        "            torch.tensor(target, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.float),\n",
        "        )\n",
        "\n",
        "train_ds = TwoTowerDataset(train_samples, news_vocab, user_vocab, cfg.max_history)\n",
        "val_ds = TwoTowerDataset(val_samples, news_vocab, user_vocab, cfg.max_history)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print('train batches:', len(train_loader), 'val batches:', len(val_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7.3) Train loop ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)\n",
        "\n",
        "model = TwoTowerModel(\n",
        "    num_users=len(user_vocab),\n",
        "    num_news=len(news_vocab),\n",
        "    embedding_dim=cfg.two_tower_embedding_dim,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "best_val = 1e9\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for (uid, hist, mask, tgt, y) in tqdm(train_loader, desc=f'two-tower train e{epoch+1}/{cfg.epochs}'):\n",
        "        uid = uid.to(device)\n",
        "        hist = hist.to(device)\n",
        "        mask = mask.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logits, _, _ = model(uid, hist, mask, tgt)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total += float(loss.detach().cpu().item())\n",
        "\n",
        "    avg_train = total / max(1, len(train_loader))\n",
        "\n",
        "    model.eval()\n",
        "    vtotal = 0.0\n",
        "    with torch.no_grad():\n",
        "        for (uid, hist, mask, tgt, y) in tqdm(val_loader, desc=f'two-tower val e{epoch+1}/{cfg.epochs}'):\n",
        "            uid = uid.to(device)\n",
        "            hist = hist.to(device)\n",
        "            mask = mask.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, _, _ = model(uid, hist, mask, tgt)\n",
        "            loss = criterion(logits, y)\n",
        "            vtotal += float(loss.detach().cpu().item())\n",
        "\n",
        "    avg_val = vtotal / max(1, len(val_loader))\n",
        "    print(f'[TwoTower] epoch={epoch+1} train={avg_train:.4f} val={avg_val:.4f}')\n",
        "\n",
        "    # save best\n",
        "    if avg_val < best_val:\n",
        "        best_val = avg_val\n",
        "        torch.save(model.state_dict(), MODELS_DIR / 'two_tower_epoch_latest.pt')\n",
        "        print('  ✅ saved best to models/two_tower_epoch_latest.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 8) 导出 item_embeddings.npy（用于 FAISS） ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    emb = model.news_encoder.news_embedding.weight.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "np.save(DATA_DIR / 'item_embeddings.npy', emb)\n",
        "print('saved:', DATA_DIR / 'item_embeddings.npy', 'shape=', emb.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 9) 构建 FAISS index + id mapping（ivf_pq 默认，适合未来扩展；小规模也可改成 flat） ---\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "emb = np.load(DATA_DIR / 'item_embeddings.npy').astype(np.float32)\n",
        "# L2 normalize for cosine similarity (IndexFlatIP / IVF* with IP)\n",
        "norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
        "emb = emb / (norms + 1e-10)\n",
        "\n",
        "dim = emb.shape[1]\n",
        "\n",
        "n = emb.shape[0]\n",
        "nlist = max(64, min(int(np.sqrt(n) * 4), 2048))\n",
        "\n",
        "m = 8\n",
        "if dim % m != 0:\n",
        "    # choose a divisor\n",
        "    for cand in [4, 8, 16, 32]:\n",
        "        if dim % cand == 0:\n",
        "            m = cand\n",
        "            break\n",
        "\n",
        "print('faiss config:', {'dim': dim, 'n': n, 'nlist': nlist, 'm': m})\n",
        "\n",
        "quantizer = faiss.IndexFlatIP(dim)\n",
        "index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, 8, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "print('training index...')\n",
        "index.train(emb)\n",
        "print('adding...')\n",
        "index.add(emb)\n",
        "index.nprobe = 16\n",
        "\n",
        "faiss_path = MODELS_DIR / 'faiss_ivf_pq.index'\n",
        "faiss.write_index(index, str(faiss_path))\n",
        "print('saved:', faiss_path, 'ntotal=', index.ntotal)\n",
        "\n",
        "# mapping\n",
        "idx_to_news_id = {v: k for k, v in news_vocab.items()}\n",
        "import pickle\n",
        "map_path = MODELS_DIR / 'faiss_id_mapping.pkl'\n",
        "with open(map_path, 'wb') as f:\n",
        "    pickle.dump({'news_vocab': news_vocab, 'idx_to_news_id': idx_to_news_id}, f)\n",
        "print('saved:', map_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 10)（可选）训练 Phoenix：多动作 heads 同时训练（click/like/reply/repost） ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PhoenixRanker(nn.Module):\n",
        "    def __init__(self, num_news, embedding_dim=256, num_heads=8, num_layers=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.news_embedding = nn.Embedding(num_news, embedding_dim)\n",
        "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=embedding_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.click_head = nn.Linear(embedding_dim, 1)\n",
        "        self.like_head = nn.Linear(embedding_dim, 1)\n",
        "        self.reply_head = nn.Linear(embedding_dim, 1)\n",
        "        self.repost_head = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def create_isolation_mask(self, history_len, num_candidates):\n",
        "        total_len = history_len + num_candidates\n",
        "        mask = torch.zeros((total_len, total_len), dtype=torch.float)\n",
        "        cand_region = torch.full((num_candidates, num_candidates), float('-inf'))\n",
        "        cand_region.fill_diagonal_(0.0)\n",
        "        mask[history_len:, history_len:] = cand_region\n",
        "        mask[:history_len, history_len:] = float('-inf')\n",
        "        return mask\n",
        "\n",
        "    def forward(self, history_ids, candidate_ids):\n",
        "        bsz, hist_len = history_ids.shape\n",
        "        _, cand_len = candidate_ids.shape\n",
        "        input_ids = torch.cat([history_ids, candidate_ids], dim=1)\n",
        "        seq_len = input_ids.shape[1]\n",
        "        x = self.news_embedding(input_ids)\n",
        "        pos = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        x = self.dropout(x + self.position_embedding(pos))\n",
        "        attn_mask = self.create_isolation_mask(hist_len, cand_len).to(input_ids.device)\n",
        "        out = self.transformer(x, mask=attn_mask)\n",
        "        out = self.ln_f(out)\n",
        "        cand_out = out[:, hist_len:, :]\n",
        "        return {\n",
        "            'click': self.click_head(cand_out).squeeze(-1),\n",
        "            'like': self.like_head(cand_out).squeeze(-1),\n",
        "            'reply': self.reply_head(cand_out).squeeze(-1),\n",
        "            'repost': self.repost_head(cand_out).squeeze(-1),\n",
        "        }\n",
        "\n",
        "if not cfg.train_phoenix:\n",
        "    print('cfg.train_phoenix is False, skip')\n",
        "else:\n",
        "    # 训练样本：直接用 user_actions 记录，按 action type 给对应 head 打 label\n",
        "    ACTION_TO_HEAD = {\n",
        "        'click': 'click',\n",
        "        'like': 'like',\n",
        "        'reply': 'reply',\n",
        "        'repost': 'repost',\n",
        "        'quote': 'repost',\n",
        "    }\n",
        "\n",
        "    phoenix_samples = []\n",
        "    for uid, events in by_user.items():\n",
        "        history = []\n",
        "        history_set = set()\n",
        "        for ts, act, pid in events:\n",
        "            head = ACTION_TO_HEAD.get(act)\n",
        "            if head:\n",
        "                y = {'click': 0.0, 'like': 0.0, 'reply': 0.0, 'repost': 0.0}\n",
        "                y[head] = 1.0\n",
        "                phoenix_samples.append({\n",
        "                    'history': history[-cfg.max_history:],\n",
        "                    'candidate_id': pid,\n",
        "                    'y': y,\n",
        "                })\n",
        "            if pid not in history_set:\n",
        "                history.append(pid)\n",
        "                history_set.add(pid)\n",
        "\n",
        "    random.shuffle(phoenix_samples)\n",
        "    split = int(len(phoenix_samples) * 0.95)\n",
        "    ph_train = phoenix_samples[:split]\n",
        "    ph_val = phoenix_samples[split:]\n",
        "    print('phoenix samples:', len(phoenix_samples), 'train:', len(ph_train), 'val:', len(ph_val))\n",
        "\n",
        "    class PhoenixDataset(Dataset):\n",
        "        def __init__(self, samples, news_vocab, max_history):\n",
        "            self.samples = samples\n",
        "            self.news_vocab = news_vocab\n",
        "            self.max_history = max_history\n",
        "            self.unk = news_vocab.get('<UNK>', 1)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.samples)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            s = self.samples[idx]\n",
        "            hist = [self.news_vocab.get(pid, self.unk) for pid in (s.get('history') or [])]\n",
        "            if len(hist) > self.max_history:\n",
        "                hist = hist[-self.max_history:]\n",
        "            else:\n",
        "                hist = hist + [0] * (self.max_history - len(hist))\n",
        "\n",
        "            cand = self.news_vocab.get(s['candidate_id'], self.unk)\n",
        "            y = s['y']\n",
        "            return (\n",
        "                torch.tensor(hist, dtype=torch.long),\n",
        "                torch.tensor([cand], dtype=torch.long),\n",
        "                torch.tensor([y['click']], dtype=torch.float),\n",
        "                torch.tensor([y['like']], dtype=torch.float),\n",
        "                torch.tensor([y['reply']], dtype=torch.float),\n",
        "                torch.tensor([y['repost']], dtype=torch.float),\n",
        "            )\n",
        "\n",
        "    ph_train_loader = DataLoader(PhoenixDataset(ph_train, news_vocab, cfg.max_history), batch_size=cfg.phoenix_batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    ph_val_loader = DataLoader(PhoenixDataset(ph_val, news_vocab, cfg.max_history), batch_size=cfg.phoenix_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    ph_model = PhoenixRanker(\n",
        "        num_news=len(news_vocab),\n",
        "        embedding_dim=cfg.phoenix_embedding_dim,\n",
        "        num_heads=cfg.phoenix_num_heads,\n",
        "        num_layers=cfg.phoenix_num_layers,\n",
        "        dropout=0.1,\n",
        "    ).to(device)\n",
        "\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    opt = optim.AdamW(ph_model.parameters(), lr=cfg.phoenix_lr)\n",
        "\n",
        "    use_amp = torch.cuda.is_available()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    # head weights：回复/转发通常稀疏，略放大\n",
        "    w = {'click': 1.0, 'like': 2.0, 'reply': 3.0, 'repost': 3.0}\n",
        "\n",
        "    best = 1e9\n",
        "    for epoch in range(cfg.phoenix_epochs):\n",
        "        ph_model.train()\n",
        "        total = 0.0\n",
        "        for (hist, cand, y_click, y_like, y_reply, y_repost) in tqdm(ph_train_loader, desc=f'phoenix train e{epoch+1}/{cfg.phoenix_epochs}'):\n",
        "            hist = hist.to(device)\n",
        "            cand = cand.to(device)\n",
        "            y_click = y_click.to(device)\n",
        "            y_like = y_like.to(device)\n",
        "            y_reply = y_reply.to(device)\n",
        "            y_repost = y_repost.to(device)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                out = ph_model(hist, cand)\n",
        "                # [B,1]\n",
        "                lc = bce(out['click'].flatten(), y_click.flatten()) * w['click']\n",
        "                ll = bce(out['like'].flatten(), y_like.flatten()) * w['like']\n",
        "                lr = bce(out['reply'].flatten(), y_reply.flatten()) * w['reply']\n",
        "                lrp = bce(out['repost'].flatten(), y_repost.flatten()) * w['repost']\n",
        "                loss = lc + ll + lr + lrp\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            total += float(loss.detach().cpu().item())\n",
        "\n",
        "        avg_train = total / max(1, len(ph_train_loader))\n",
        "\n",
        "        ph_model.eval()\n",
        "        vtotal = 0.0\n",
        "        with torch.no_grad():\n",
        "            for (hist, cand, y_click, y_like, y_reply, y_repost) in tqdm(ph_val_loader, desc=f'phoenix val e{epoch+1}/{cfg.phoenix_epochs}'):\n",
        "                hist = hist.to(device)\n",
        "                cand = cand.to(device)\n",
        "                y_click = y_click.to(device)\n",
        "                y_like = y_like.to(device)\n",
        "                y_reply = y_reply.to(device)\n",
        "                y_repost = y_repost.to(device)\n",
        "                out = ph_model(hist, cand)\n",
        "                lc = bce(out['click'].flatten(), y_click.flatten()) * w['click']\n",
        "                ll = bce(out['like'].flatten(), y_like.flatten()) * w['like']\n",
        "                lr = bce(out['reply'].flatten(), y_reply.flatten()) * w['reply']\n",
        "                lrp = bce(out['repost'].flatten(), y_repost.flatten()) * w['repost']\n",
        "                loss = lc + ll + lr + lrp\n",
        "                vtotal += float(loss.detach().cpu().item())\n",
        "\n",
        "        avg_val = vtotal / max(1, len(ph_val_loader))\n",
        "        print(f'[Phoenix] epoch={epoch+1} train={avg_train:.4f} val={avg_val:.4f}')\n",
        "\n",
        "        if avg_val < best:\n",
        "            best = avg_val\n",
        "            torch.save(ph_model.state_dict(), MODELS_DIR / 'phoenix_epoch_latest.pt')\n",
        "            print('  ✅ saved best to models/phoenix_epoch_latest.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 11) 生成可上传的 stage/ 目录（结构对齐你的 GCS artifacts 约定） ---\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "stage = Path('stage')\n",
        "if stage.exists():\n",
        "    shutil.rmtree(stage)\n",
        "\n",
        "(stage / 'two_tower').mkdir(parents=True, exist_ok=True)\n",
        "(stage / 'phoenix').mkdir(parents=True, exist_ok=True)\n",
        "(stage / 'faiss').mkdir(parents=True, exist_ok=True)\n",
        "(stage / 'data').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 必需\n",
        "shutil.copy2(MODELS_DIR / 'two_tower_epoch_latest.pt', stage / 'two_tower' / 'model.pt')\n",
        "\n",
        "phoenix_path = MODELS_DIR / 'phoenix_epoch_latest.pt'\n",
        "if phoenix_path.exists():\n",
        "    shutil.copy2(phoenix_path, stage / 'phoenix' / 'model.pt')\n",
        "else:\n",
        "    print('phoenix model missing, you can still upload two-tower + faiss and keep phoenix disabled on server')\n",
        "\n",
        "shutil.copy2(MODELS_DIR / 'faiss_ivf_pq.index', stage / 'faiss' / 'faiss_ivf_pq.index')\n",
        "shutil.copy2(MODELS_DIR / 'faiss_id_mapping.pkl', stage / 'faiss' / 'faiss_id_mapping.pkl')\n",
        "\n",
        "# 数据\n",
        "shutil.copy2(DATA_DIR / 'news_vocab.pkl', stage / 'data' / 'news_vocab.pkl')\n",
        "shutil.copy2(DATA_DIR / 'user_vocab.pkl', stage / 'data' / 'user_vocab.pkl')\n",
        "shutil.copy2(DATA_DIR / 'item_embeddings.npy', stage / 'data' / 'item_embeddings.npy')\n",
        "\n",
        "print('stage ready:', stage)\n",
        "for p in stage.rglob('*'):\n",
        "    if p.is_file():\n",
        "        print(' -', p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 12) （可选）打包 stage，便于从 Colab 下载到本地再上传到 GCS ---\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "\n",
        "bundle = Path('stage_bundle.tgz')\n",
        "if bundle.exists():\n",
        "    bundle.unlink()\n",
        "\n",
        "with tarfile.open(bundle, 'w:gz') as tar:\n",
        "    tar.add('stage', arcname='stage')\n",
        "\n",
        "size_mb = bundle.stat().st_size / (1024 * 1024)\n",
        "print('bundle:', bundle, f'{size_mb:.2f} MB')\n",
        "print('下载方式：Colab 左侧 Files 面板里找到 stage_bundle.tgz -> 右键 Download')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 上传到 GCS（在你本地电脑执行，沿用你之前成功的流程）\n",
        "\n",
        "1) 选择新版本号：`ARTIFACT_VERSION=YYYY-MM-DD_buildNN`，例如：`2026-02-07_build02`\n",
        "\n",
        "2) 把 stage 上传到 bucket：\n",
        "\n",
        "```bash\n",
        "BUCKET=\"telegram-467705-recsys\"\n",
        "ARTIFACT_VERSION=\"2026-02-07_build02\"\n",
        "\n",
        "# 将 stage/ 目录上传到 artifacts/$ARTIFACT_VERSION/\n",
        "gcloud storage cp -r stage/* \"gs://$BUCKET/artifacts/$ARTIFACT_VERSION/\"\n",
        "\n",
        "# 校验\n",
        "gcloud storage ls -r \"gs://$BUCKET/artifacts/$ARTIFACT_VERSION/**\"\n",
        "```\n",
        "\n",
        "3) 更新 Cloud Run：\n",
        "\n",
        "```bash\n",
        "gcloud run services update telegram-ml-services \\\n",
        "  --project telegram-467705 \\\n",
        "  --region us-central1 \\\n",
        "  --update-env-vars \"ARTIFACT_VERSION=$ARTIFACT_VERSION,TWO_TOWER_EMBEDDING_DIM=256,PHOENIX_EMBEDDING_DIM=256,PHOENIX_NUM_HEADS=8,PHOENIX_NUM_LAYERS=4\"\n",
        "```\n",
        "\n",
        "4) 验证：\n",
        "\n",
        "- `GET /health` 应显示 `models_loaded=true`、`faiss_enabled=true`\n",
        "- `POST /ann/retrieve` 返回的 `postId` 应该是 24 位 hex 的 ObjectId（而不是 `Nxxxxx`）\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}