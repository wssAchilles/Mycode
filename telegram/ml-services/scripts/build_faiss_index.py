"""
FAISS ç´¢å¼•æ„å»ºè„šæœ¬
å¤åˆ» x-algorithm çš„å‘é‡æ£€ç´¢ä¼˜åŒ–

æ”¯æŒçš„ç´¢å¼•ç±»å‹:
- Flat: ç²¾ç¡®æœç´¢ (å°è§„æ¨¡)
- IVF: å€’æ’ç´¢å¼• (ä¸­ç­‰è§„æ¨¡)
- HNSW: å›¾ç´¢å¼• (é«˜å¬å›ç‡)
- IVF+PQ: é‡åŒ–å‹ç¼© (å¤§è§„æ¨¡)
"""

import faiss
import numpy as np
import pickle
from pathlib import Path
from typing import Literal
import time
import argparse

# è·¯å¾„é…ç½®
DATA_DIR = Path(__file__).parent.parent / "data"
MODELS_DIR = Path(__file__).parent.parent / "models"

# ç´¢å¼•ç±»å‹
IndexType = Literal["flat", "ivf", "hnsw", "ivf_pq"]


def load_embeddings():
    """åŠ è½½ item embeddings"""
    embeddings_path = DATA_DIR / "item_embeddings.npy"
    if not embeddings_path.exists():
        raise FileNotFoundError(
            f"Item embeddings not found at {embeddings_path}. "
            "Please run train_two_tower.py first."
        )
    
    embeddings = np.load(embeddings_path).astype(np.float32)
    print(f"âœ… Loaded embeddings: shape={embeddings.shape}")
    return embeddings


def load_id_mapping():
    """åŠ è½½ news_vocab (id -> index æ˜ å°„)"""
    vocab_path = DATA_DIR / "news_vocab.pkl"
    if not vocab_path.exists():
        raise FileNotFoundError(f"News vocab not found at {vocab_path}")
    
    with open(vocab_path, "rb") as f:
        news_vocab = pickle.load(f)
    
    # åå‘æ˜ å°„: index -> news_id
    idx_to_news_id = {v: k for k, v in news_vocab.items()}
    print(f"âœ… Loaded ID mapping: {len(idx_to_news_id)} items")
    return news_vocab, idx_to_news_id


def normalize_embeddings(embeddings: np.ndarray) -> np.ndarray:
    """L2 å½’ä¸€åŒ– (ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦)"""
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    norms = np.where(norms == 0, 1, norms)  # é¿å…é™¤é›¶
    return embeddings / norms


def build_flat_index(embeddings: np.ndarray) -> faiss.Index:
    """
    Flat Index - ç²¾ç¡®æœç´¢
    é€‚ç”¨: <10ä¸‡å‘é‡
    """
    dim = embeddings.shape[1]
    # ä½¿ç”¨ IP (å†…ç§¯) é…åˆå½’ä¸€åŒ–å‘é‡ = ä½™å¼¦ç›¸ä¼¼åº¦
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)
    print(f"ğŸ“¦ Built Flat index with {index.ntotal} vectors")
    return index


def build_ivf_index(
    embeddings: np.ndarray, 
    nlist: int = 100,
    nprobe: int = 10
) -> faiss.Index:
    """
    IVF Index - å€’æ’æ–‡ä»¶ç´¢å¼•
    é€‚ç”¨: 10ä¸‡-100ä¸‡å‘é‡
    
    Args:
        nlist: èšç±»ä¸­å¿ƒæ•°é‡ (æ¨è sqrt(n) åˆ° 4*sqrt(n))
        nprobe: æœç´¢æ—¶æ£€æŸ¥çš„èšç±»æ•° (è¶Šå¤§è¶Šå‡†ä½†è¶Šæ…¢)
    """
    dim = embeddings.shape[1]
    quantizer = faiss.IndexFlatIP(dim)
    index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_INNER_PRODUCT)
    
    # è®­ç»ƒèšç±»ä¸­å¿ƒ
    print(f"ğŸ”„ Training IVF index with {nlist} clusters...")
    index.train(embeddings)
    index.add(embeddings)
    index.nprobe = nprobe
    
    print(f"ğŸ“¦ Built IVF index: {index.ntotal} vectors, nlist={nlist}, nprobe={nprobe}")
    return index


def build_hnsw_index(
    embeddings: np.ndarray,
    M: int = 32,
    efConstruction: int = 200,
    efSearch: int = 64
) -> faiss.Index:
    """
    HNSW Index - å±‚æ¬¡å¯å¯¼èˆªå°ä¸–ç•Œå›¾
    é€‚ç”¨: éœ€è¦é«˜å¬å›ç‡åœºæ™¯
    
    Args:
        M: æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…æ•° (16-64)
        efConstruction: æ„å»ºæ—¶çš„æœç´¢å®½åº¦
        efSearch: æŸ¥è¯¢æ—¶çš„æœç´¢å®½åº¦
    """
    dim = embeddings.shape[1]
    index = faiss.IndexHNSWFlat(dim, M, faiss.METRIC_INNER_PRODUCT)
    index.hnsw.efConstruction = efConstruction
    index.hnsw.efSearch = efSearch
    
    print(f"ğŸ”„ Building HNSW index (M={M}, efConstruction={efConstruction})...")
    index.add(embeddings)
    
    print(f"ğŸ“¦ Built HNSW index: {index.ntotal} vectors")
    return index


def build_ivf_pq_index(
    embeddings: np.ndarray,
    nlist: int = 256,
    m: int = 8,  # å­å‘é‡æ•°é‡
    nbits: int = 8,  # æ¯ä¸ªå­å‘é‡çš„é‡åŒ–ä½æ•°
    nprobe: int = 16
) -> faiss.Index:
    """
    IVF+PQ Index - å€’æ’ç´¢å¼• + ä¹˜ç§¯é‡åŒ–
    é€‚ç”¨: >100ä¸‡å‘é‡, éœ€è¦èŠ‚çœå†…å­˜
    
    Args:
        nlist: èšç±»ä¸­å¿ƒæ•°é‡
        m: å­å‘é‡æ•°é‡ (å¿…é¡»èƒ½æ•´é™¤ dim)
        nbits: æ¯ä¸ªå­å‘é‡çš„é‡åŒ–ä½æ•° (8=256ä¸ªèšç±»ä¸­å¿ƒ)
        nprobe: æœç´¢æ—¶æ£€æŸ¥çš„èšç±»æ•°
    """
    dim = embeddings.shape[1]
    
    # ç¡®ä¿ m èƒ½æ•´é™¤ dim
    if dim % m != 0:
        m = min([i for i in [4, 8, 16, 32] if dim % i == 0], default=dim)
        print(f"âš ï¸ Adjusted m to {m} (must divide dim={dim})")
    
    quantizer = faiss.IndexFlatIP(dim)
    index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, nbits, faiss.METRIC_INNER_PRODUCT)
    
    print(f"ğŸ”„ Training IVF+PQ index (nlist={nlist}, m={m}, nbits={nbits})...")
    index.train(embeddings)
    index.add(embeddings)
    index.nprobe = nprobe
    
    print(f"ğŸ“¦ Built IVF+PQ index: {index.ntotal} vectors")
    return index


def benchmark_index(index: faiss.Index, embeddings: np.ndarray, k: int = 100):
    """æ€§èƒ½æµ‹è¯•"""
    # éšæœºé€‰æ‹© 100 ä¸ªæŸ¥è¯¢å‘é‡
    n_queries = min(100, embeddings.shape[0])
    query_indices = np.random.choice(embeddings.shape[0], n_queries, replace=False)
    queries = embeddings[query_indices]
    
    # é¢„çƒ­
    index.search(queries[:10], k)
    
    # è®¡æ—¶
    start = time.time()
    distances, indices = index.search(queries, k)
    elapsed = time.time() - start
    
    qps = n_queries / elapsed
    avg_latency = elapsed / n_queries * 1000  # ms
    
    # å¬å›ç‡ä¼°ç®— (è‡ªæ£€ç´¢åº”è¯¥è¿”å›è‡ªå·±)
    recall_at_1 = np.mean(indices[:, 0] == query_indices)
    
    print(f"ğŸ“Š Benchmark Results:")
    print(f"   QPS: {qps:.1f}")
    print(f"   Avg Latency: {avg_latency:.2f} ms")
    print(f"   Recall@1 (self): {recall_at_1:.2%}")
    
    return {"qps": qps, "latency_ms": avg_latency, "recall_at_1": recall_at_1}


def save_index(index: faiss.Index, index_type: str):
    """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
    MODELS_DIR.mkdir(exist_ok=True)
    index_path = MODELS_DIR / f"faiss_{index_type}.index"
    faiss.write_index(index, str(index_path))
    
    # æ–‡ä»¶å¤§å°
    size_mb = index_path.stat().st_size / (1024 * 1024)
    print(f"ğŸ’¾ Saved index to {index_path} ({size_mb:.2f} MB)")
    
    return index_path


def build_index(
    index_type: IndexType = "ivf",
    normalize: bool = True,
    benchmark: bool = True
) -> Path:
    """
    ä¸»æ„å»ºå‡½æ•°
    
    Args:
        index_type: ç´¢å¼•ç±»å‹
        normalize: æ˜¯å¦ L2 å½’ä¸€åŒ–
        benchmark: æ˜¯å¦è¿è¡Œæ€§èƒ½æµ‹è¯•
    """
    print(f"\n{'='*50}")
    print(f"ğŸš€ Building FAISS Index: {index_type.upper()}")
    print(f"{'='*50}\n")
    
    # 1. åŠ è½½æ•°æ®
    embeddings = load_embeddings()
    news_vocab, idx_to_news_id = load_id_mapping()
    
    # 2. å½’ä¸€åŒ–
    if normalize:
        print("ğŸ”„ Normalizing embeddings...")
        embeddings = normalize_embeddings(embeddings)
    
    # 3. æ„å»ºç´¢å¼•
    start_time = time.time()
    
    if index_type == "flat":
        index = build_flat_index(embeddings)
    elif index_type == "ivf":
        # è‡ªåŠ¨è®¡ç®— nlist
        n = embeddings.shape[0]
        nlist = max(16, min(int(np.sqrt(n) * 2), 1024))
        index = build_ivf_index(embeddings, nlist=nlist)
    elif index_type == "hnsw":
        index = build_hnsw_index(embeddings)
    elif index_type == "ivf_pq":
        n = embeddings.shape[0]
        nlist = max(64, min(int(np.sqrt(n) * 4), 2048))
        index = build_ivf_pq_index(embeddings, nlist=nlist)
    else:
        raise ValueError(f"Unknown index type: {index_type}")
    
    build_time = time.time() - start_time
    print(f"â±ï¸ Build time: {build_time:.2f} seconds")
    
    # 4. æ€§èƒ½æµ‹è¯•
    if benchmark:
        benchmark_index(index, embeddings)
    
    # 5. ä¿å­˜ç´¢å¼•
    index_path = save_index(index, index_type)
    
    # 6. ä¿å­˜ ID æ˜ å°„
    mapping_path = MODELS_DIR / "faiss_id_mapping.pkl"
    with open(mapping_path, "wb") as f:
        pickle.dump({
            "news_vocab": news_vocab,
            "idx_to_news_id": idx_to_news_id
        }, f)
    print(f"ğŸ’¾ Saved ID mapping to {mapping_path}")
    
    print(f"\n{'='*50}")
    print(f"âœ… FAISS Index build complete!")
    print(f"{'='*50}\n")
    
    return index_path


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build FAISS index for recommendation")
    parser.add_argument(
        "--type", 
        type=str, 
        default="ivf",
        choices=["flat", "ivf", "hnsw", "ivf_pq"],
        help="Index type (default: ivf)"
    )
    parser.add_argument(
        "--no-normalize",
        action="store_true",
        help="Skip L2 normalization"
    )
    parser.add_argument(
        "--no-benchmark",
        action="store_true",
        help="Skip benchmark"
    )
    
    args = parser.parse_args()
    
    build_index(
        index_type=args.type,
        normalize=not args.no_normalize,
        benchmark=not args.no_benchmark
    )
