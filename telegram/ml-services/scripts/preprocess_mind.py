"""
MIND æ•°æ®é›†é¢„å¤„ç†è„šæœ¬
å°† MIND-Small æ•°æ®é›†è½¬æ¢ä¸º PyTorch å¯ç”¨çš„æ ¼å¼
"""

import os
import pickle
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

# è·¯å¾„é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent.parent
TRAIN_DIR = PROJECT_ROOT / "MINDsmall_train"
DEV_DIR = PROJECT_ROOT / "MINDsmall_dev"
OUTPUT_DIR = Path(__file__).parent.parent / "data"


def load_news(news_path: Path) -> Dict[str, dict]:
    """
    åŠ è½½ news.tsv æ–‡ä»¶
    æ ¼å¼: news_id \t category \t subcategory \t title \t abstract \t url \t title_entities \t abstract_entities
    """
    print(f"ğŸ“° åŠ è½½æ–°é—»æ•°æ®: {news_path}")
    
    # è¯»å– TSV æ–‡ä»¶
    df = pd.read_csv(
        news_path,
        sep="\t",
        header=None,
        names=["news_id", "category", "subcategory", "title", "abstract", "url", "title_entities", "abstract_entities"],
        dtype=str,
        na_values=[""],
    )
    
    # å¡«å……ç¼ºå¤±å€¼
    df = df.fillna("")
    
    # è½¬æ¢ä¸ºå­—å…¸
    news_dict = {}
    for _, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†æ–°é—»"):
        news_dict[row["news_id"]] = {
            "news_id": row["news_id"],
            "category": row["category"],
            "subcategory": row["subcategory"],
            "title": row["title"],
            "abstract": row["abstract"],
            "text": f"{row['title']} {row['abstract']}",  # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦
        }
    
    print(f"  âœ… åŠ è½½äº† {len(news_dict)} æ¡æ–°é—»")
    return news_dict


def load_behaviors(behaviors_path: Path) -> List[dict]:
    """
    åŠ è½½ behaviors.tsv æ–‡ä»¶
    æ ¼å¼: impression_id \t user_id \t time \t history \t impressions
    """
    print(f"ğŸ‘¤ åŠ è½½ç”¨æˆ·è¡Œä¸ºæ•°æ®: {behaviors_path}")
    
    # è¯»å– TSV æ–‡ä»¶
    df = pd.read_csv(
        behaviors_path,
        sep="\t",
        header=None,
        names=["impression_id", "user_id", "time", "history", "impressions"],
        dtype=str,
        na_values=[""],
    )
    
    # å¡«å……ç¼ºå¤±å€¼
    df = df.fillna("")
    
    behaviors = []
    for _, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è¡Œä¸º"):
        # è§£æå†å²ç‚¹å‡»
        history = row["history"].split() if row["history"] else []
        
        # è§£ææ›å…‰å’Œç‚¹å‡»
        # æ ¼å¼: news_id-click (1=ç‚¹å‡», 0=æœªç‚¹å‡»)
        impressions = []
        if row["impressions"]:
            for imp in row["impressions"].split():
                parts = imp.rsplit("-", 1)
                if len(parts) == 2:
                    news_id, click = parts
                    impressions.append({
                        "news_id": news_id,
                        "clicked": click == "1"
                    })
        
        behaviors.append({
            "impression_id": row["impression_id"],
            "user_id": row["user_id"],
            "time": row["time"],
            "history": history,
            "impressions": impressions,
        })
    
    print(f"  âœ… åŠ è½½äº† {len(behaviors)} æ¡è¡Œä¸ºè®°å½•")
    return behaviors


def create_user_sequences(behaviors: List[dict], news_dict: Dict[str, dict]) -> Dict[str, List[str]]:
    """
    ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºå†å²ç‚¹å‡»åºåˆ—
    """
    print("ğŸ”„ åˆ›å»ºç”¨æˆ·åºåˆ—...")
    
    user_sequences = {}
    for behavior in tqdm(behaviors, desc="æ„å»ºåºåˆ—"):
        user_id = behavior["user_id"]
        if user_id not in user_sequences:
            user_sequences[user_id] = []
        
        # æ·»åŠ å†å²è®°å½•
        user_sequences[user_id].extend(behavior["history"])
        
        # æ·»åŠ å½“å‰ç‚¹å‡»
        for imp in behavior["impressions"]:
            if imp["clicked"]:
                user_sequences[user_id].append(imp["news_id"])
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    for user_id in user_sequences:
        seen = set()
        unique = []
        for news_id in user_sequences[user_id]:
            if news_id not in seen and news_id in news_dict:
                seen.add(news_id)
                unique.append(news_id)
        user_sequences[user_id] = unique
    
    print(f"  âœ… åˆ›å»ºäº† {len(user_sequences)} ä¸ªç”¨æˆ·åºåˆ—")
    return user_sequences


def create_training_samples(behaviors: List[dict], news_dict: Dict[str, dict]) -> List[dict]:
    """
    åˆ›å»ºè®­ç»ƒæ ·æœ¬ (æ­£æ ·æœ¬ + è´Ÿæ ·æœ¬)
    æ¯æ¡æ ·æœ¬: {user_id, user_history, candidate_news_id, label}
    """
    print("ğŸ“Š åˆ›å»ºè®­ç»ƒæ ·æœ¬...")
    
    samples = []
    for behavior in tqdm(behaviors, desc="ç”Ÿæˆæ ·æœ¬"):
        user_id = behavior["user_id"]
        history = [nid for nid in behavior["history"] if nid in news_dict]
        
        if len(history) < 3:  # å†å²å¤ªçŸ­åˆ™è·³è¿‡
            continue
        
        for imp in behavior["impressions"]:
            news_id = imp["news_id"]
            if news_id not in news_dict:
                continue
            
            samples.append({
                "user_id": user_id,
                "history": history[-50:],  # åªä¿ç•™æœ€è¿‘50æ¡
                "candidate_id": news_id,
                "label": 1 if imp["clicked"] else 0,
            })
    
    print(f"  âœ… ç”Ÿæˆäº† {len(samples)} æ¡è®­ç»ƒæ ·æœ¬")
    
    # ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
    pos = sum(1 for s in samples if s["label"] == 1)
    neg = len(samples) - pos
    print(f"  ğŸ“ˆ æ­£æ ·æœ¬: {pos} ({pos/len(samples)*100:.1f}%)")
    print(f"  ğŸ“‰ è´Ÿæ ·æœ¬: {neg} ({neg/len(samples)*100:.1f}%)")
    
    return samples


def build_vocabularies(news_dict: Dict[str, dict], user_sequences: Dict[str, List[str]]) -> Tuple[Dict, Dict]:
    """
    æ„å»º news_id å’Œ user_id çš„æ˜ å°„è¯è¡¨
    """
    print("ğŸ“– æ„å»ºè¯è¡¨...")
    
    # News ID è¯è¡¨
    news_vocab = {"<PAD>": 0, "<UNK>": 1}
    for news_id in news_dict.keys():
        if news_id not in news_vocab:
            news_vocab[news_id] = len(news_vocab)
    
    # User ID è¯è¡¨
    user_vocab = {"<PAD>": 0, "<UNK>": 1}
    for user_id in user_sequences.keys():
        if user_id not in user_vocab:
            user_vocab[user_id] = len(user_vocab)
    
    print(f"  âœ… News è¯è¡¨å¤§å°: {len(news_vocab)}")
    print(f"  âœ… User è¯è¡¨å¤§å°: {len(user_vocab)}")
    
    return news_vocab, user_vocab


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ MIND æ•°æ®é¢„å¤„ç†å¼€å§‹")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½è®­ç»ƒé›†æ–°é—»
    train_news = load_news(TRAIN_DIR / "news.tsv")
    dev_news = load_news(DEV_DIR / "news.tsv")
    
    # åˆå¹¶æ–°é—»å­—å…¸
    all_news = {**train_news, **dev_news}
    print(f"ğŸ“° æ€»æ–°é—»æ•°: {len(all_news)}")
    
    # åŠ è½½è¡Œä¸ºæ•°æ®
    train_behaviors = load_behaviors(TRAIN_DIR / "behaviors.tsv")
    dev_behaviors = load_behaviors(DEV_DIR / "behaviors.tsv")
    
    # åˆ›å»ºç”¨æˆ·åºåˆ—
    user_sequences = create_user_sequences(train_behaviors + dev_behaviors, all_news)
    
    # åˆ›å»ºè®­ç»ƒæ ·æœ¬
    train_samples = create_training_samples(train_behaviors, all_news)
    dev_samples = create_training_samples(dev_behaviors, all_news)
    
    # æ„å»ºè¯è¡¨
    news_vocab, user_vocab = build_vocabularies(all_news, user_sequences)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    print("\nğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
    
    with open(OUTPUT_DIR / "news_dict.pkl", "wb") as f:
        pickle.dump(all_news, f)
    
    with open(OUTPUT_DIR / "user_sequences.pkl", "wb") as f:
        pickle.dump(user_sequences, f)
    
    with open(OUTPUT_DIR / "train_samples.pkl", "wb") as f:
        pickle.dump(train_samples, f)
    
    with open(OUTPUT_DIR / "dev_samples.pkl", "wb") as f:
        pickle.dump(dev_samples, f)
    
    with open(OUTPUT_DIR / "news_vocab.pkl", "wb") as f:
        pickle.dump(news_vocab, f)
    
    with open(OUTPUT_DIR / "user_vocab.pkl", "wb") as f:
        pickle.dump(user_vocab, f)
    
    print("\n" + "=" * 60)
    print("âœ… é¢„å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("=" * 60)


if __name__ == "__main__":
    main()
