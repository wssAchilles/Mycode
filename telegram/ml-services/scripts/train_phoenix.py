import torch
from torch.utils.data import Dataset, DataLoader
import pickle
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
from tqdm import tqdm
from phoenix_model import PhoenixRanker
import torch.cuda.amp as amp # 1. å¼•å…¥æ··åˆç²¾åº¦æ¨¡å—

# é…ç½® (Max Scale for Colab Pro)
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = Path(__file__).parent.parent / "data"
MODELS_DIR = Path(__file__).parent.parent / "models"

# ğŸš€ H100 (80GB VRAM) MAX PERFORMANCE Configuration
BATCH_SIZE = 1536       # 1024å ç”¨42GBï¼Œ1536é¢„è®¡å ç”¨63GB (å®‰å…¨è·‘æ»¡)
EMBEDDING_DIM = 768    
NUM_HEADS = 12         
NUM_LAYERS = 12        
EPOCHS = 10            
LR = 5e-5              
MAX_HISTORY = 100      
NUM_CANDIDATES = 20    

# è®¾å¤‡
if torch.backends.mps.is_available():
    device = torch.device("mps")
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

class PhoenixDataset(Dataset):
    def __init__(self, samples_path, news_vocab, user_vocab):
        # å¤ç”¨ Two-Tower çš„ samples æ ¼å¼ï¼Œä½†éœ€è¦åœ¨ getitem æ—¶åŠ¨æ€æ„é€ å¤šå€™é€‰(Positive + Negatives)
        with open(samples_path, "rb") as f:
            self.samples = pickle.load(f)
        self.news_vocab = news_vocab
        self.num_news = len(news_vocab)
        
        # é¢„è®¡ç®—æ‰€æœ‰æ–°é—»IDåˆ—è¡¨ç”¨äºè´Ÿé‡‡æ ·
        self.all_news_ids = list(news_vocab.values())
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆ Listwise è®­ç»ƒæ•°æ®æ„é€ 
        # åŸå§‹ samples æ˜¯ pointwise (1ä¸ªæ ·æœ¬1ä¸ªlabel)
        # æˆ‘ä»¬è¿™é‡Œåªå– Positive æ ·æœ¬ï¼Œç„¶åéšæœºé‡‡æ · Negatives
        
        sample = self.samples[idx]
        target_id = self.news_vocab.get(sample['candidate_id'], 0)
        label = sample['label']
        
        # Pointwise Training, Batch Size = Bã€‚
        # æ­¤æ—¶ Phoenix è¾“å…¥ Candidates é•¿åº¦ä¸º 1ã€‚
        
        history_ids = [self.news_vocab.get(nid, 0) for nid in sample['history']]
        if len(history_ids) > MAX_HISTORY:
            history_ids = history_ids[-MAX_HISTORY:]
        else:
            history_ids = history_ids + [0] * (MAX_HISTORY - len(history_ids))
            
        return {
            "history": torch.tensor(history_ids, dtype=torch.long),
            "candidate": torch.tensor([target_id], dtype=torch.long), # [1]
            "label": torch.tensor(float(label), dtype=torch.float)
        }

def train():
    # 0. æ˜¾å­˜å¤§æ‰«é™¤ (é˜²æ­¢æ®‹ç•™)
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print("ğŸ“– Loading vocab...")
    with open(DATA_DIR / "news_vocab.pkl", "rb") as f:
        news_vocab = pickle.load(f)
        
    print("preparing dataset...")
    dataset = PhoenixDataset(DATA_DIR / "train_samples.pkl", news_vocab, {})
    
    # å¯ç”¨å¤šè¿›ç¨‹åŠ è½½ (num_workers) å’Œ é”é¡µå†…å­˜ (pin_memory)
    loader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True,
        num_workers=2,        # å›é€€åˆ° 4 (8 æ ¸å¿ƒå¯èƒ½å¯¼è‡´æ­»é”/å¡ä½)
        pin_memory=True,      # åŠ é€Ÿæ•°æ®ä¼ è¾“åˆ° GPU
        persistent_workers=True # ä¿æŒè¿›ç¨‹æ´»è·ƒ
    )
    
    print("ğŸ”§ Init Phoenix Model...")
    model = PhoenixRanker(
        num_news=len(news_vocab),
        embedding_dim=EMBEDDING_DIM,
        num_heads=NUM_HEADS,
        num_layers=NUM_LAYERS
    ).to(device)
    
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    
    # 2. åˆå§‹åŒ– AMP Scaler
    scaler = amp.GradScaler()
    print("âš¡ Mixed Precision Training (AMP) Enabled")
    
    model.train()
    print(f"ğŸ”¥ Start Phoenix Training (Batch Size: {BATCH_SIZE})...")
    
    for epoch in range(EPOCHS):
        total_loss = 0
        pbar = tqdm(loader, desc=f"Epoch {epoch+1}")
        
        for batch in pbar:
            history = batch['history'].to(device) # [B, HistLen]
            candidate = batch['candidate'].to(device) # [B, 1]
            label = batch['label'].to(device) # [B]
            
            optimizer.zero_grad()
            
            # 3. ä½¿ç”¨ Autocast è‡ªåŠ¨æ··åˆç²¾åº¦
            with amp.autocast():
                # Forward
                outputs = model(history, candidate)
                logits = outputs['click'].squeeze(-1) # [B, 1] -> [B, 1] or [B]??
                loss = criterion(logits.flatten(), label)
            
            # 4. ä½¿ç”¨ Scaler è¿›è¡Œåå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            total_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
        avg_loss = total_loss / len(loader)
        print(f"âœ… Epoch {epoch+1} finished. Loss: {avg_loss:.4f}")
        
        # Save
        torch.save(model.state_dict(), MODELS_DIR / f"phoenix_epoch_{epoch+1}.pt")

if __name__ == "__main__":
    train()
