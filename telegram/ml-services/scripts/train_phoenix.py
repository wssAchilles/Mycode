import torch
from torch.utils.data import Dataset, DataLoader
import pickle
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
from tqdm import tqdm
from phoenix_model import PhoenixRanker

# é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = Path(__file__).parent.parent / "data"
MODELS_DIR = Path(__file__).parent.parent / "models"
BATCH_SIZE = 64 # Transformer æ˜¾å­˜å ç”¨å¤§ï¼Œè°ƒå° Batch
EMBEDDING_DIM = 256
NUM_HEADS = 4
NUM_LAYERS = 2 # å¿«é€Ÿè®­ç»ƒ demo (å®žé™…å¯ç”¨ 4-6)
EPOCHS = 3
LR = 0.0001
MAX_HISTORY = 40
NUM_CANDIDATES = 5 # æ¯æ¬¡è®­ç»ƒé‡‡æ · 1æ­£ + 4è´Ÿ

# è®¾å¤‡
if torch.backends.mps.is_available():
    device = torch.device("mps")
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

class PhoenixDataset(Dataset):
    def __init__(self, samples_path, news_vocab, user_vocab):
        # å¤ç”¨ Two-Tower çš„ samples æ ¼å¼ï¼Œä½†éœ€è¦åœ¨ getitem æ—¶åŠ¨æ€æž„é€ å¤šå€™é€‰(Positive + Negatives)
        with open(samples_path, "rb") as f:
            self.samples = pickle.load(f)
        self.news_vocab = news_vocab
        self.num_news = len(news_vocab)
        
        # é¢„è®¡ç®—æ‰€æœ‰æ–°é—»IDåˆ—è¡¨ç”¨äºŽè´Ÿé‡‡æ ·
        self.all_news_ids = list(news_vocab.values())
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆ Listwise è®­ç»ƒæ•°æ®æž„é€ 
        # åŽŸå§‹ samples æ˜¯ pointwise (1ä¸ªæ ·æœ¬1ä¸ªlabel)
        # æˆ‘ä»¬è¿™é‡Œåªå– Positive æ ·æœ¬ï¼Œç„¶åŽéšæœºé‡‡æ · Negatives
        
        # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ä»éåŽ† samplesã€‚å¦‚æžœæ˜¯ Positiveï¼Œå°±éšæœºé‡‡è´Ÿã€‚
        # å¦‚æžœæ˜¯ Negative æ ·æœ¬... è¿™é‡Œä¸ºäº†æ¼”ç¤º Candidate Isolationï¼Œæˆ‘ä»¬å¼ºåˆ¶æž„é€  listã€‚
        
        sample = self.samples[idx]
        target_id = self.news_vocab.get(sample['candidate_id'], 0)
        label = sample['label']
        
        # ç®€å• Hack: åªæ‹¿ Label=1 çš„æ•°æ®æ¥è®­ç»ƒ List æŽ’åº?
        # è¿™æ ·ä¼šä¸¢å¼ƒ dataset é‡ŒåŽŸæœ¬çš„ Label=0 çš„å¼ºè´Ÿä¾‹ï¼ˆImpressions but not clickedï¼‰ã€‚
        # æ­£ç¡®åšæ³•ï¼šæŒ‰ Session/ImpressionID èšåˆã€‚
        # ä½† preprocess_mind output å·²ç»æ˜¯ flat samplesã€‚
        # å¦¥åï¼šæ¯æ¬¡åªè®­ç»ƒ 1 ä¸ª candidate (Pointwise)ï¼Œä½†ä¾ç„¶èµ° Phoenix æž¶æž„ (num_candidates=1)ã€‚
        # æˆ–è€…ï¼šåœ¨çº¿éšæœºè´Ÿé‡‡æ ·ã€‚
        
        # è®©æˆ‘ä»¬åš Pointwise ä½†æ”¯æŒ batch ç»´åº¦æ‰©å±• (è¿™é‡Œ num_candidates=1)
        # è¿™æ ·ä»£ç ç®€å•ï¼Œä¸”èƒ½åˆ©ç”¨çŽ°æœ‰çš„ samplesã€‚ Isolation Mask æ­¤æ—¶é€€åŒ–ä¸ºæ— ã€‚
        
        # **ä¿®æ­£**ï¼šä¸ºäº†å±•ç¤º Phoenix "è¯„åˆ†å¤šä¸ªå€™é€‰" çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ Inference æ—¶ä¼šä¼ å…¥å¤šä¸ªã€‚
        # åœ¨ Training æ—¶ï¼Œå¦‚æžœæˆ‘ä»¬ç”¨ Pointwise Loss (BCE)ï¼Œæˆ‘ä»¬åªéœ€è¦ä¼  1 ä¸ª candidateã€‚
        # å¦‚æžœç”¨ Listwise Loss (InfoNCE / Softmax)ï¼Œæˆ‘ä»¬éœ€è¦å¤šä¸ªã€‚
        # è¿™é‡Œä¸ºäº†ç¨³å®šï¼Œä½¿ç”¨ Pointwise Training, Batch Size = Bã€‚
        # æ­¤æ—¶ Phoenix è¾“å…¥ Candidates é•¿åº¦ä¸º 1ã€‚
        
        history_ids = [self.news_vocab.get(nid, 0) for nid in sample['history']]
        if len(history_ids) > MAX_HISTORY:
            history_ids = history_ids[-MAX_HISTORY:]
        else:
            history_ids = history_ids + [0] * (MAX_HISTORY - len(history_ids))
            
        return {
            "history": torch.tensor(history_ids, dtype=torch.long),
            "candidate": torch.tensor([target_id], dtype=torch.long), # [1]
            "label": torch.tensor(float(label), dtype=torch.float)
        }

def train():
    print("ðŸ“– Loading vocab...")
    with open(DATA_DIR / "news_vocab.pkl", "rb") as f:
        news_vocab = pickle.load(f)
        
    print("preparing dataset...")
    dataset = PhoenixDataset(DATA_DIR / "train_samples.pkl", news_vocab, {})
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    print("ðŸ”§ Init Phoenix Model...")
    model = PhoenixRanker(
        num_news=len(news_vocab),
        embedding_dim=EMBEDDING_DIM,
        num_heads=NUM_HEADS,
        num_layers=NUM_LAYERS
    ).to(device)
    
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    
    model.train()
    print("ðŸ”¥ Start Phoenix Training...")
    
    for epoch in range(EPOCHS):
        total_loss = 0
        pbar = tqdm(loader, desc=f"Epoch {epoch+1}")
        
        for batch in pbar:
            history = batch['history'].to(device) # [B, HistLen]
            candidate = batch['candidate'].to(device) # [B, 1]
            label = batch['label'].to(device) # [B]
            
            # Forward
            # output dict keys: click, like, etc.
            # æˆ‘ä»¬åªç”¨ 'click' head å¯¹åº” click label
            outputs = model(history, candidate)
            logits = outputs['click'].squeeze(-1) # [B, 1] -> [B, 1] or [B]??
            # model output shape: [B, num_cands]. here num_cands=1. -> [B, 1]
            
            loss = criterion(logits.flatten(), label)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
        avg_loss = total_loss / len(loader)
        print(f"âœ… Epoch {epoch+1} finished. Loss: {avg_loss:.4f}")
        
        # Save
        torch.save(model.state_dict(), MODELS_DIR / f"phoenix_epoch_{epoch+1}.pt")

if __name__ == "__main__":
    train()
