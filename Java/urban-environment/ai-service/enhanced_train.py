#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºç‰ˆAIæ¨¡å‹è®­ç»ƒè„šæœ¬ - enhanced_train.py
åŠŸèƒ½ï¼š
1. æ”¯æŒå¤šä¼ æ„Ÿå™¨ç±»å‹æ•°æ®
2. é«˜çº§ç‰¹å¾å·¥ç¨‹ï¼ˆæ—¶é—´åºåˆ—ç‰¹å¾ã€åœ°ç†ç‰¹å¾ç­‰ï¼‰
3. å¤šç§å¼‚å¸¸æ£€æµ‹ç®—æ³•å¯¹æ¯”
4. æ¨¡å‹é›†æˆå’Œä¼˜åŒ–
5. å®æ—¶é¢„è­¦é˜ˆå€¼è°ƒä¼˜
"""

import os
import pandas as pd
import numpy as np
import psycopg2
import joblib
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from dotenv import load_dotenv
import logging
from typing import Optional, Dict, Tuple, List
import warnings
from datetime import datetime, timedelta

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EnvironmentalAnomalyDetector:
    """ç¯å¢ƒæ•°æ®å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        self.thresholds = {}
        
    def load_environment(self) -> Dict:
        """åŠ è½½ç¯å¢ƒå˜é‡"""
        load_dotenv()
        
        db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'database': os.getenv('DB_NAME', 'urban_environment_db'),
            'user': os.getenv('DB_USER', 'user'),
            'password': os.getenv('DB_PASSWORD', 'password'),
            'port': os.getenv('DB_PORT', '5433')
        }
        
        logger.info(f"æ•°æ®åº“é…ç½®: {db_config['host']}:{db_config['port']}/{db_config['database']}")
        return db_config
    
    def fetch_enhanced_data(self, db_config: Dict) -> Optional[pd.DataFrame]:
        """
        è·å–å¢å¼ºçš„ä¼ æ„Ÿå™¨æ•°æ®ï¼ŒåŒ…å«æ›´å¤šç‰¹å¾
        """
        try:
            logger.info("æ­£åœ¨è¿æ¥æ•°æ®åº“è·å–å¢å¼ºæ•°æ®...")
            
            conn = psycopg2.connect(**db_config)
            
            # è·å–æœ€è¿‘7å¤©çš„æ•°æ®ç”¨äºç‰¹å¾å·¥ç¨‹
            query = """
            SELECT 
                id, device_id, 
                pm25, latitude, longitude, 
                timestamp,
                EXTRACT(HOUR FROM timestamp) as hour_of_day,
                EXTRACT(DOW FROM timestamp) as day_of_week,
                EXTRACT(DOY FROM timestamp) as day_of_year
            FROM sensor_data 
            WHERE pm25 IS NOT NULL 
                AND timestamp >= NOW() - INTERVAL '7 days'
            ORDER BY device_id, timestamp
            """
            
            df = pd.read_sql_query(query, conn)
            conn.close()
            
            if len(df) == 0:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ€è¿‘7å¤©çš„æ•°æ®ï¼Œä½¿ç”¨æ‰€æœ‰å†å²æ•°æ®")
                return self.fetch_all_data(db_config)
            
            logger.info(f"è·å–åˆ° {len(df)} æ¡å¢å¼ºæ•°æ®")
            return df
            
        except Exception as e:
            logger.error(f"è·å–å¢å¼ºæ•°æ®å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€æ•°æ®è·å–
            return self.fetch_all_data(db_config)
    
    def fetch_all_data(self, db_config: Dict) -> Optional[pd.DataFrame]:
        """è·å–æ‰€æœ‰å†å²æ•°æ®ä½œä¸ºå›é€€æ–¹æ¡ˆ"""
        try:
            conn = psycopg2.connect(**db_config)
            
            query = """
            SELECT 
                id, device_id, 
                pm25, latitude, longitude, 
                timestamp
            FROM sensor_data 
            WHERE pm25 IS NOT NULL 
            ORDER BY timestamp DESC
            LIMIT 10000
            """
            
            df = pd.read_sql_query(query, conn)
            conn.close()
            
            # æ·»åŠ æ—¶é—´ç‰¹å¾
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['hour_of_day'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            df['day_of_year'] = df['timestamp'].dt.dayofyear
            
            logger.info(f"å›é€€è·å–åˆ° {len(df)} æ¡åŸºç¡€æ•°æ®")
            return df
            
        except Exception as e:
            logger.error(f"è·å–åŸºç¡€æ•°æ®ä¹Ÿå¤±è´¥: {e}")
            return None
    
    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é«˜çº§ç‰¹å¾å·¥ç¨‹
        """
        logger.info("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        
        # ç¡®ä¿timestampæ˜¯datetimeç±»å‹
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # æŒ‰è®¾å¤‡åˆ†ç»„è¿›è¡Œç‰¹å¾å·¥ç¨‹
        enhanced_df = []
        
        for device_id in df['device_id'].unique():
            device_data = df[df['device_id'] == device_id].copy().sort_values('timestamp')
            
            if len(device_data) < 2:
                continue
                
            # 1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            device_data['pm25_mean'] = device_data['pm25'].rolling(window=5, min_periods=1).mean()
            device_data['pm25_std'] = device_data['pm25'].rolling(window=5, min_periods=1).std()
            device_data['pm25_min'] = device_data['pm25'].rolling(window=5, min_periods=1).min()
            device_data['pm25_max'] = device_data['pm25'].rolling(window=5, min_periods=1).max()
            
            # 2. å˜åŒ–ç‡ç‰¹å¾
            device_data['pm25_diff'] = device_data['pm25'].diff()
            device_data['pm25_pct_change'] = device_data['pm25'].pct_change()
            
            # 3. æ—¶é—´åºåˆ—ç‰¹å¾
            device_data['pm25_lag1'] = device_data['pm25'].shift(1)
            device_data['pm25_lag2'] = device_data['pm25'].shift(2)
            
            # 4. åœ°ç†ä½ç½®åç§»ç‰¹å¾ï¼ˆå¦‚æœè®¾å¤‡ç§»åŠ¨ï¼‰
            device_data['lat_diff'] = device_data['latitude'].diff()
            device_data['lon_diff'] = device_data['longitude'].diff()
            device_data['location_change'] = np.sqrt(
                device_data['lat_diff']**2 + device_data['lon_diff']**2
            )
            
            # 5. æ—¶é—´å‘¨æœŸç‰¹å¾
            device_data['hour_sin'] = np.sin(2 * np.pi * device_data['hour_of_day'] / 24)
            device_data['hour_cos'] = np.cos(2 * np.pi * device_data['hour_of_day'] / 24)
            device_data['day_sin'] = np.sin(2 * np.pi * device_data['day_of_week'] / 7)
            device_data['day_cos'] = np.cos(2 * np.pi * device_data['day_of_week'] / 7)
            
            enhanced_df.append(device_data)
        
        if not enhanced_df:
            logger.error("ç‰¹å¾å·¥ç¨‹å¤±è´¥ï¼šæ²¡æœ‰è¶³å¤Ÿçš„è®¾å¤‡æ•°æ®")
            return df
        
        result_df = pd.concat(enhanced_df, ignore_index=True)
        
        # å¡«å……NaNå€¼
        result_df = result_df.fillna(method='ffill').fillna(0)
        
        # å®šä¹‰ç‰¹å¾åˆ—
        self.feature_columns = [
            'pm25', 'pm25_mean', 'pm25_std', 'pm25_min', 'pm25_max',
            'pm25_diff', 'pm25_pct_change', 'pm25_lag1', 'pm25_lag2',
            'latitude', 'longitude', 'location_change',
            'hour_sin', 'hour_cos', 'day_sin', 'day_cos'
        ]
        
        # è¿‡æ»¤æ‰æ— ç©·å¤§å’ŒNaNå€¼
        for col in self.feature_columns:
            if col in result_df.columns:
                result_df[col] = result_df[col].replace([np.inf, -np.inf], np.nan)
                result_df[col] = result_df[col].fillna(result_df[col].median())
        
        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç”Ÿæˆ {len(self.feature_columns)} ä¸ªç‰¹å¾")
        return result_df
    
    def train_multiple_models(self, df: pd.DataFrame) -> Dict:
        """
        è®­ç»ƒå¤šç§å¼‚å¸¸æ£€æµ‹æ¨¡å‹
        """
        logger.info("å¼€å§‹è®­ç»ƒå¤šç§å¼‚å¸¸æ£€æµ‹æ¨¡å‹...")
        
        # å‡†å¤‡ç‰¹å¾æ•°æ®
        X = df[self.feature_columns].values
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = RobustScaler()  # ä½¿ç”¨RobustScalerå¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥
        X_scaled = scaler.fit_transform(X)
        self.scalers['robust'] = scaler
        
        models = {}
        
        # 1. Isolation Forest (ä¸»æ¨è)
        logger.info("è®­ç»ƒ Isolation Forest...")
        iso_forest = IsolationForest(
            contamination=0.1,  # å‡è®¾10%çš„æ•°æ®æ˜¯å¼‚å¸¸
            random_state=42,
            n_estimators=200,
            max_samples='auto',
            max_features=0.8,
            bootstrap=False,
            n_jobs=-1
        )
        iso_forest.fit(X_scaled)
        models['isolation_forest'] = iso_forest
        
        # 2. One-Class SVM
        logger.info("è®­ç»ƒ One-Class SVM...")
        ocsvm = OneClassSVM(
            kernel='rbf',
            gamma='scale',
            nu=0.1  # å¼‚å¸¸å€¼æ¯”ä¾‹
        )
        ocsvm.fit(X_scaled)
        models['one_class_svm'] = ocsvm
        
        # 3. Elliptic Envelope (é²æ£’åæ–¹å·®ä¼°è®¡)
        logger.info("è®­ç»ƒ Elliptic Envelope...")
        elliptic = EllipticEnvelope(
            contamination=0.1,
            random_state=42
        )
        elliptic.fit(X_scaled)
        models['elliptic_envelope'] = elliptic
        
        # 4. Local Outlier Factor (ä»…ç”¨äºé¢„æµ‹ï¼Œä¸ä¿å­˜fitçŠ¶æ€)
        logger.info("é…ç½® Local Outlier Factor...")
        lof = LocalOutlierFactor(
            contamination=0.1,
            novelty=True  # å…è®¸é¢„æµ‹æ–°æ•°æ®
        )
        lof.fit(X_scaled)
        models['local_outlier_factor'] = lof
        
        self.models = models
        
        # è¯„ä¼°æ¨¡å‹æ€§èƒ½
        self.evaluate_models(X_scaled, models)
        
        return models
    
    def evaluate_models(self, X: np.ndarray, models: Dict) -> None:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        """
        logger.info("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        results = {}
        
        for name, model in models.items():
            predictions = model.predict(X)
            anomaly_count = sum(predictions == -1)
            anomaly_rate = anomaly_count / len(predictions) * 100
            
            results[name] = {
                'anomaly_count': anomaly_count,
                'anomaly_rate': anomaly_rate
            }
            
            logger.info(f"{name}: æ£€æµ‹åˆ° {anomaly_count} ä¸ªå¼‚å¸¸ ({anomaly_rate:.2f}%)")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ (è¿™é‡Œé€‰æ‹©Isolation Forestä½œä¸ºä¸»æ¨¡å‹)
        self.best_model = 'isolation_forest'
        logger.info(f"é€‰æ‹© {self.best_model} ä½œä¸ºä¸»è¦æ¨¡å‹")
    
    def create_ensemble_model(self) -> None:
        """
        åˆ›å»ºé›†æˆæ¨¡å‹ï¼Œç»“åˆå¤šä¸ªç®—æ³•çš„é¢„æµ‹ç»“æœ
        """
        logger.info("åˆ›å»ºé›†æˆå¼‚å¸¸æ£€æµ‹æ¨¡å‹...")
        
        def ensemble_predict(X):
            """é›†æˆé¢„æµ‹å‡½æ•°"""
            predictions = {}
            scores = {}
            
            # è·å–å„æ¨¡å‹çš„é¢„æµ‹ç»“æœ
            for name, model in self.models.items():
                pred = model.predict(X)
                predictions[name] = pred
                
                # è·å–å¼‚å¸¸åˆ†æ•°
                if hasattr(model, 'decision_function'):
                    scores[name] = model.decision_function(X)
                elif hasattr(model, 'score_samples'):
                    scores[name] = model.score_samples(X)
                else:
                    scores[name] = pred  # ä½¿ç”¨é¢„æµ‹ç»“æœä½œä¸ºåˆ†æ•°
            
            # æŠ•ç¥¨æœºåˆ¶ï¼šå¤šæ•°æ¨¡å‹è®¤ä¸ºæ˜¯å¼‚å¸¸åˆ™ä¸ºå¼‚å¸¸
            ensemble_pred = []
            ensemble_score = []
            
            for i in range(len(X)):
                votes = [predictions[name][i] for name in predictions]
                anomaly_votes = sum(1 for vote in votes if vote == -1)
                
                # å¦‚æœè¶…è¿‡ä¸€åŠçš„æ¨¡å‹è®¤ä¸ºæ˜¯å¼‚å¸¸ï¼Œåˆ™åˆ¤å®šä¸ºå¼‚å¸¸
                if anomaly_votes >= len(self.models) / 2:
                    ensemble_pred.append(-1)
                else:
                    ensemble_pred.append(1)
                
                # å¹³å‡å¼‚å¸¸åˆ†æ•°
                avg_score = np.mean([scores[name][i] for name in scores])
                ensemble_score.append(avg_score)
            
            return np.array(ensemble_pred), np.array(ensemble_score)
        
        self.ensemble_predict = ensemble_predict
    
    def optimize_thresholds(self, df: pd.DataFrame) -> None:
        """
        ä¼˜åŒ–å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
        """
        logger.info("ä¼˜åŒ–å¼‚å¸¸æ£€æµ‹é˜ˆå€¼...")
        
        X = df[self.feature_columns].values
        X_scaled = self.scalers['robust'].transform(X)
        
        # ä½¿ç”¨ä¸»æ¨¡å‹è·å–å†³ç­–åˆ†æ•°
        main_model = self.models[self.best_model]
        scores = main_model.decision_function(X_scaled)
        
        # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ç»Ÿè®¡ä¿¡æ¯
        percentiles = [90, 95, 97, 99, 99.5]
        
        for p in percentiles:
            threshold = np.percentile(scores, 100 - p)
            anomaly_count = sum(scores < threshold)
            anomaly_rate = anomaly_count / len(scores) * 100
            
            self.thresholds[f'p{p}'] = {
                'threshold': threshold,
                'anomaly_rate': anomaly_rate
            }
            
            logger.info(f"é˜ˆå€¼ P{p}: {threshold:.4f} (å¼‚å¸¸ç‡: {anomaly_rate:.2f}%)")
        
        # è®¾ç½®é»˜è®¤é˜ˆå€¼ (95th percentile)
        self.default_threshold = self.thresholds['p95']['threshold']
    
    def save_enhanced_model(self, model_dir: str) -> bool:
        """
        ä¿å­˜å¢å¼ºçš„æ¨¡å‹å’Œç›¸å…³ç»„ä»¶
        """
        try:
            os.makedirs(model_dir, exist_ok=True)
            
            # ä¿å­˜ä¸»æ¨¡å‹
            main_model_path = os.path.join(model_dir, 'enhanced_anomaly_model.joblib')
            joblib.dump(self.models[self.best_model], main_model_path)
            
            # ä¿å­˜æ‰€æœ‰æ¨¡å‹
            all_models_path = os.path.join(model_dir, 'all_models.joblib')
            joblib.dump(self.models, all_models_path)
            
            # ä¿å­˜ç¼©æ”¾å™¨
            scaler_path = os.path.join(model_dir, 'feature_scaler.joblib')
            joblib.dump(self.scalers['robust'], scaler_path)
            
            # ä¿å­˜ç‰¹å¾åˆ—å’Œé˜ˆå€¼ä¿¡æ¯
            metadata = {
                'feature_columns': self.feature_columns,
                'thresholds': self.thresholds,
                'default_threshold': self.default_threshold,
                'best_model': self.best_model,
                'training_time': datetime.now().isoformat()
            }
            
            metadata_path = os.path.join(model_dir, 'model_metadata.joblib')
            joblib.dump(metadata, metadata_path)
            
            # åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
            info_path = os.path.join(model_dir, 'model_info.txt')
            with open(info_path, 'w', encoding='utf-8') as f:
                f.write("Enhanced Environmental Anomaly Detection Model\n")
                f.write("=" * 50 + "\n\n")
                f.write(f"Training Time: {metadata['training_time']}\n")
                f.write(f"Best Model: {self.best_model}\n")
                f.write(f"Features: {len(self.feature_columns)}\n")
                f.write(f"Feature List:\n")
                for i, feature in enumerate(self.feature_columns, 1):
                    f.write(f"  {i}. {feature}\n")
                f.write(f"\nThresholds:\n")
                for name, info in self.thresholds.items():
                    f.write(f"  {name}: {info['threshold']:.4f} ({info['anomaly_rate']:.2f}%)\n")
            
            logger.info(f"å¢å¼ºæ¨¡å‹ä¿å­˜æˆåŠŸåˆ°: {model_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜å¢å¼ºæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def train_complete_pipeline(self) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµæ°´çº¿
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹å¢å¼ºç‰ˆAIå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
        logger.info("=" * 60)
        
        # 1. åŠ è½½ç¯å¢ƒé…ç½®
        db_config = self.load_environment()
        
        # 2. è·å–å¢å¼ºæ•°æ®
        df = self.fetch_enhanced_data(db_config)
        if df is None or len(df) == 0:
            logger.error("æ— æ³•è·å–è®­ç»ƒæ•°æ®")
            return False
        
        logger.info(f"æ•°æ®æ¦‚è§ˆ:")
        logger.info(f"  è®°å½•æ•°: {len(df)}")
        logger.info(f"  è®¾å¤‡æ•°: {df['device_id'].nunique()}")
        logger.info(f"  PM2.5èŒƒå›´: {df['pm25'].min():.2f} - {df['pm25'].max():.2f}")
        logger.info(f"  æ—¶é—´èŒƒå›´: {df['timestamp'].min()} åˆ° {df['timestamp'].max()}")
        
        # 3. ç‰¹å¾å·¥ç¨‹
        df_enhanced = self.engineer_features(df)
        if len(df_enhanced) < 10:
            logger.error("ç‰¹å¾å·¥ç¨‹åæ•°æ®ä¸è¶³")
            return False
        
        # 4. è®­ç»ƒå¤šç§æ¨¡å‹
        models = self.train_multiple_models(df_enhanced)
        
        # 5. åˆ›å»ºé›†æˆæ¨¡å‹
        self.create_ensemble_model()
        
        # 6. ä¼˜åŒ–é˜ˆå€¼
        self.optimize_thresholds(df_enhanced)
        
        # 7. ä¿å­˜æ¨¡å‹
        script_dir = os.path.dirname(os.path.abspath(__file__))
        model_dir = os.path.join(script_dir, "models")
        success = self.save_enhanced_model(model_dir)
        
        if success:
            logger.info("=" * 60)
            logger.info("ğŸ‰ å¢å¼ºç‰ˆAIæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            logger.info("âœ… ç‰¹å¾å·¥ç¨‹: æ—¶é—´åºåˆ— + åœ°ç† + ç»Ÿè®¡ç‰¹å¾")
            logger.info("âœ… å¤šç®—æ³•é›†æˆ: IsolationForest + OneClassSVM + EllipticEnvelope + LOF")
            logger.info("âœ… æ™ºèƒ½é˜ˆå€¼ä¼˜åŒ–: è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹é˜ˆå€¼")
            logger.info("âœ… å®Œæ•´æ¨¡å‹åŒ…: æ¨¡å‹ + ç¼©æ”¾å™¨ + å…ƒæ•°æ®")
            logger.info("ğŸš€ æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼")
            logger.info("=" * 60)
            return True
        else:
            logger.error("æ¨¡å‹ä¿å­˜å¤±è´¥")
            return False


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    detector = EnvironmentalAnomalyDetector()
    success = detector.train_complete_pipeline()
    return success


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)